{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1-b89Fwyt4py4qaiLXqkDTdkIYTqFftys",
      "authorship_tag": "ABX9TyO4Yb556hWce+QQjNpYtw7y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aadityakhant/fakeNewsDetection/blob/main/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U \"tensorflow-text==2.8.*\" --quiet\n",
        "!pip install -q -U tf-models-official==2.7.0 --quiet\n",
        "!pip install -U tfds-nightly --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7qGCHlZg752",
        "outputId": "2fb03527-0dc3-4d7e-9a8a-0b90139001b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-gbq 0.19.2 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "tfds-nightly 4.9.4.dev202405020044 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-gbq 0.19.2 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "tensorflow 2.8.4 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, display\n",
        "def progress(value, max=100):\n",
        "    return HTML(\"\"\"\n",
        "        <progress\n",
        "            value='{value}'\n",
        "            max='{max}',\n",
        "            style='width: 100%'\n",
        "        >\n",
        "            {value}\n",
        "        </progress>\n",
        "    \"\"\".format(value=value, max=max))"
      ],
      "metadata": {
        "id": "rCDRFRmzIVjw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "LVxlEp_m-hA8",
        "outputId": "e68591b3-811a-4f20-e178-412fa7bc1cac"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "        <progress\n",
              "            value='321'\n",
              "            max='50',\n",
              "            style='width: 100%'\n",
              "        >\n",
              "            321\n",
              "        </progress>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import os\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as text  # A dependency of the preprocessing model\n",
        "import tensorflow_addons as tfa\n",
        "from official.nlp import optimization\n",
        "import numpy as np\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "out = display(progress(0, 25000), display_id=True)\n",
        "\n",
        "def make_bert_preprocess_model(sentence_features, seq_length=128):\n",
        "\n",
        "  input_segments = [\n",
        "      tf.keras.layers.Input(shape=(), dtype=tf.string, name=ft)\n",
        "      for ft in sentence_features]\n",
        "\n",
        "  # Tokenize the text to word pieces.\n",
        "  bert_preprocess = hub.load('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')\n",
        "  tokenizer = hub.KerasLayer(bert_preprocess.tokenize, name='tokenizer')\n",
        "  truncated_segments = [tokenizer(s) for s in input_segments]\n",
        "  packer = hub.KerasLayer(bert_preprocess.bert_pack_inputs,\n",
        "                          arguments=dict(seq_length=seq_length),\n",
        "                          name='packer')\n",
        "  model_inputs = packer(truncated_segments)\n",
        "  return tf.keras.Model(input_segments, model_inputs)\n",
        "\n",
        "\n",
        "bert_preprocess_model = make_bert_preprocess_model(['title', 'content'])\n",
        "\n",
        "def build_classifier_model(num_classes):\n",
        "\n",
        "  class Classifier(tf.keras.Model):\n",
        "    def __init__(self, num_classes):\n",
        "      super(Classifier, self).__init__(name=\"prediction\")\n",
        "      self.encoder = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1', trainable=True)\n",
        "      self.dropout = tf.keras.layers.Dropout(0.1)\n",
        "      self.dense = tf.keras.layers.Dense(num_classes)\n",
        "\n",
        "    def call(self, preprocessed_text):\n",
        "      encoder_outputs = self.encoder(preprocessed_text)\n",
        "      pooled_output = encoder_outputs[\"pooled_output\"]\n",
        "      x = self.dropout(pooled_output)\n",
        "      x = self.dense(x)\n",
        "      return x\n",
        "\n",
        "  model = Classifier(num_classes)\n",
        "  return model\n",
        "\n",
        "def data_loader(l, r, length, bert_preprocess_model):\n",
        "  classes = {'fake':0, 'clickbait':1, 'valid':2, 'bias':3}\n",
        "  news={'input_mask':[], 'input_type_ids':[], 'input_word_ids':[]}\n",
        "  labels = []\n",
        "  for i in range(l,r):\n",
        "      out.update(progress(i, 50))\n",
        "      file = open('./drive/MyDrive/data2/pickle-'+str(i)+'.csv', 'r', encoding='latin-1')\n",
        "      reader = csv.reader(file)\n",
        "      for r in reader:\n",
        "        pre = bert_preprocess_model([np.array([r[0]]), np.array([r[1]])])\n",
        "        news['input_mask'].append(pre['input_mask'])\n",
        "        news['input_type_ids'].append(pre['input_type_ids'])\n",
        "        news['input_word_ids'].append(pre['input_word_ids'])\n",
        "        labels.append(classes[r[2]])\n",
        "      file.close()\n",
        "  news['input_mask'] = tf.reshape(news['input_mask'], (length, 128))\n",
        "  news['input_type_ids'] = tf.reshape(news['input_type_ids'], (length, 128))\n",
        "  news['input_word_ids'] = tf.reshape(news['input_word_ids'], (length, 128))\n",
        "  return (news,labels)\n",
        "\n",
        "news_train = data_loader(0,50,25000,bert_preprocess_model)\n",
        "news_valid = data_loader(320,322,1000,bert_preprocess_model)\n",
        "\n",
        "def load_dataset_from_tfds(in_memory_ds, train, batch_size, ):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(in_memory_ds)\n",
        "  if train:\n",
        "    dataset = dataset.shuffle(len(dataset))\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  dataset = dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "  return dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "batch_size = 32\n",
        "init_lr = 2e-5\n",
        "\n",
        "train_dataset = load_dataset_from_tfds(news_train, train=True, batch_size=batch_size)\n",
        "steps_per_epoch = 25000 // batch_size\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "#num_warmup_steps = num_train_steps // 10\n",
        "\n",
        "validation_dataset = load_dataset_from_tfds(news_valid, train=False, batch_size=batch_size, )\n",
        "validation_steps = 1000 // batch_size\n",
        "\n",
        "metrics = tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "classifier_model = build_classifier_model(4)\n",
        "\n",
        "optimizer = optimization.create_optimizer(\n",
        "    init_lr=init_lr,\n",
        "    num_train_steps=num_train_steps,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    optimizer_type='adamw')\n",
        "\n",
        "classifier_model.compile(optimizer=optimizer, loss=loss, metrics=[metrics])\n",
        "\n",
        "checkpoint_path = \"./drive/MyDrive/cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)"
      ],
      "metadata": {
        "id": "kQFKCsewH0RJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h = classifier_model.fit(\n",
        "      x=train_dataset,\n",
        "      validation_data=validation_dataset,\n",
        "      steps_per_epoch=steps_per_epoch,\n",
        "      epochs=epochs,\n",
        "      validation_steps=validation_steps,\n",
        "      callbacks=[cp_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZQjhmsmPWJw",
        "outputId": "c9901712-6b39-41a8-9d10-6139910f6fa2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "781/781 [==============================] - ETA: 0s - loss: 1.0012 - accuracy: 0.5651 \n",
            "Epoch 1: saving model to ./drive/MyDrive/cp.ckpt\n",
            "781/781 [==============================] - 8020s 10s/step - loss: 1.0012 - accuracy: 0.5651 - val_loss: 0.7251 - val_accuracy: 0.6875\n",
            "Epoch 2/10\n",
            "  1/781 [..............................] - ETA: 36:19 - loss: 0.9866 - accuracy: 0.6250\n",
            "Epoch 2: saving model to ./drive/MyDrive/cp.ckpt\n",
            "781/781 [==============================] - 99s 123ms/step - loss: 0.9866 - accuracy: 0.6250 - val_loss: 0.7361 - val_accuracy: 0.6885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "news_test = data_loader(360,400,20000,bert_preprocess_model)"
      ],
      "metadata": {
        "id": "CEv7ypyJyaJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = load_dataset_from_tfds(news_test, train=False, batch_size=batch_size, )\n",
        "validation_steps = 20000 // batch_size"
      ],
      "metadata": {
        "id": "Go2W81QSyfsh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_model.evaluate(test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAcvob4Jzih8",
        "outputId": "3a81adf7-47d2-42a4-cf0b-9bdb8a40c8da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "546/625 [=========================>....] - ETA: 3:33 - loss: 0.6645 - accuracy: 0.7660"
          ]
        }
      ]
    }
  ]
}