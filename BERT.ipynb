{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "138gdaOH0RbWbj5u-hO4v1U5XqX3Gegj-",
      "authorship_tag": "ABX9TyM7Uc/8AzyJzD+XDCUSjUhR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aadityakhant/fakeNewsDetection/blob/main/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U \"tensorflow-text==2.8.*\"\n",
        "!pip install -q -U tf-models-official==2.7.0\n",
        "!pip install -U tfds-nightly"
      ],
      "metadata": {
        "id": "zz0noQSqndzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as text  # A dependency of the preprocessing model\n",
        "import tensorflow_addons as tfa\n",
        "from official.nlp import optimization\n",
        "import numpy as np\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "metadata": {
        "id": "kjGA_3a8oglW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_bert_preprocess_model(sentence_features, seq_length=128):\n",
        "\n",
        "  input_segments = [\n",
        "      tf.keras.layers.Input(shape=(), dtype=tf.string, name=ft)\n",
        "      for ft in sentence_features]\n",
        "\n",
        "  # Tokenize the text to word pieces.\n",
        "  bert_preprocess = hub.load('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')\n",
        "  tokenizer = hub.KerasLayer(bert_preprocess.tokenize, name='tokenizer')\n",
        "  truncated_segments = [tokenizer(s) for s in input_segments]\n",
        "  packer = hub.KerasLayer(bert_preprocess.bert_pack_inputs,\n",
        "                          arguments=dict(seq_length=seq_length),\n",
        "                          name='packer')\n",
        "  model_inputs = packer(truncated_segments)\n",
        "  return tf.keras.Model(input_segments, model_inputs)\n",
        "\n",
        "  test_preprocess_model = make_bert_preprocess_model(['title', 'content'])\n",
        "  tf.keras.utils.plot_model(test_preprocess_model, show_shapes=True)"
      ],
      "metadata": {
        "id": "zXTTQJR3olCl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_classifier_model(num_classes):\n",
        "\n",
        "  class Classifier(tf.keras.Model):\n",
        "    def __init__(self, num_classes):\n",
        "      super(Classifier, self).__init__(name=\"prediction\")\n",
        "      self.encoder = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1', trainable=True)\n",
        "      self.dropout = tf.keras.layers.Dropout(0.1)\n",
        "      self.dense = tf.keras.layers.Dense(num_classes)\n",
        "\n",
        "    def call(self, preprocessed_text):\n",
        "      encoder_outputs = self.encoder(preprocessed_text)\n",
        "      pooled_output = encoder_outputs[\"pooled_output\"]\n",
        "      x = self.dropout(pooled_output)\n",
        "      x = self.dense(x)\n",
        "      return x\n",
        "\n",
        "  model = Classifier(num_classes)\n",
        "  return model"
      ],
      "metadata": {
        "id": "RMGpHhfxwCx7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = {'fake':0, 'clickbait':1, 'valid':2, 'bias':3}\n",
        "\n",
        "news_train={'x1':[], 'x2':[], 'y':[]}\n",
        "for i in range(360):\n",
        "    file = open('./drive/MyDrive/data2/pickle-'+str(i)+'.csv', 'r', encoding='latin-1')\n",
        "    reader = csv.reader(file)\n",
        "    for r in reader:\n",
        "      news_train['x1'].append(r[0])\n",
        "      news_train['x2'].append(r[1])\n",
        "      news_train['y'].append(classes[r[2]])\n",
        "\n",
        "news_valid={'x1':[], 'x2':[], 'y':[]}\n",
        "for i in range(320, 360):\n",
        "    file = open('./drive/MyDrive/data2/pickle-'+str(i)+'.csv', 'r', encoding='latin-1')\n",
        "    reader = csv.reader(file)\n",
        "    for r in reader:\n",
        "      news_valid['x1'].append(r[0])\n",
        "      news_valid['x2'].append(r[1])\n",
        "      news_valid['y'].append(classes[r[2]])\n",
        "\n",
        "news_test={'x1':[], 'x2':[], 'y':[]}\n",
        "for i in range(360, 400):\n",
        "    file = open('./drive/MyDrive/data2/pickle-'+str(i)+'.csv', 'r', encoding='latin-1')\n",
        "    reader = csv.reader(file)\n",
        "    for r in reader:\n",
        "      news_test['x1'].append(r[0])\n",
        "      news_test['x2'].append(r[1])\n",
        "      news_test['y'].append(classes[r[2]])"
      ],
      "metadata": {
        "id": "CfUblucUva65"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset_from_tfds(in_memory_ds, train, batch_size, bert_preprocess_model):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(in_memory_ds)\n",
        "  if train:\n",
        "    dataset = dataset.shuffle(len(dataset))\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  dataset = dataset.map(lambda x: (bert_preprocess_model([x['x1'],x['x2']]), x['y']))\n",
        "  dataset = dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "WxIVUgqnfHEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 3\n",
        "batch_size = 32\n",
        "init_lr = 2e-5\n",
        "\n",
        "bert_preprocess_model = make_bert_preprocess_model(['title', 'content'])\n",
        "\n",
        "train_dataset = load_dataset_from_tfds(\n",
        "    news_train, train=True, batch_size=batch_size,\n",
        "    bert_preprocess_model=bert_preprocess_model)\n",
        "steps_per_epoch = 160000 // batch_size\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = num_train_steps // 10\n",
        "\n",
        "validation_dataset = load_dataset_from_tfds(\n",
        "    news_valid, train=False, batch_size=batch_size,\n",
        "    bert_preprocess_model=bert_preprocess_model)\n",
        "validation_steps = 20000 // batch_size\n",
        "\n",
        "metrics = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "      'accuracy', dtype=tf.float32)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "classifier_model = build_classifier_model(4)\n",
        "\n",
        "optimizer = optimization.create_optimizer(\n",
        "    init_lr=init_lr,\n",
        "    num_train_steps=num_train_steps,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    optimizer_type='adamw')\n",
        "\n",
        "classifier_model.compile(optimizer=optimizer, loss=loss, metrics=[metrics])"
      ],
      "metadata": {
        "id": "DUsdFgTPygPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_model.fit(\n",
        "    x=validation_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=epochs,\n",
        "    validation_steps=validation_steps)"
      ],
      "metadata": {
        "id": "GM744QTeCu8P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}