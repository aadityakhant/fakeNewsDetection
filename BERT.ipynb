{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aadityakhant/fakeNewsDetection/blob/main/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U \"tensorflow-text==2.8.*\" --quiet\n",
        "!pip install -q -U tf-models-official==2.7.0 --quiet\n",
        "!pip install -U tfds-nightly --quiet"
      ],
      "metadata": {
        "id": "zz0noQSqndzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as text  # A dependency of the preprocessing model\n",
        "import tensorflow_addons as tfa\n",
        "from official.nlp import optimization\n",
        "import numpy as np\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "metadata": {
        "id": "kjGA_3a8oglW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_bert_preprocess_model(sentence_features, seq_length=128):\n",
        "\n",
        "  input_segments = [\n",
        "      tf.keras.layers.Input(shape=(), dtype=tf.string, name=ft)\n",
        "      for ft in sentence_features]\n",
        "\n",
        "  # Tokenize the text to word pieces.\n",
        "  bert_preprocess = hub.load('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')\n",
        "  tokenizer = hub.KerasLayer(bert_preprocess.tokenize, name='tokenizer')\n",
        "  truncated_segments = [tokenizer(s) for s in input_segments]\n",
        "  packer = hub.KerasLayer(bert_preprocess.bert_pack_inputs,\n",
        "                          arguments=dict(seq_length=seq_length),\n",
        "                          name='packer')\n",
        "  model_inputs = packer(truncated_segments)\n",
        "  return tf.keras.Model(input_segments, model_inputs)\n",
        "\n",
        "  test_preprocess_model = make_bert_preprocess_model(['title', 'content'])\n",
        "  tf.keras.utils.plot_model(test_preprocess_model, show_shapes=True)"
      ],
      "metadata": {
        "id": "zXTTQJR3olCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_preprocess_model = make_bert_preprocess_model(['title', 'content'])"
      ],
      "metadata": {
        "id": "USzsgVD2rj-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_preprocess_model([np.array(['title']),\n",
        "                       np.array(['content'])]).keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcsU4urmWPvo",
        "outputId": "25db755a-6df1-4c99-9b22-ffd8e68e3f85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_mask', 'input_type_ids', 'input_word_ids'])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_classifier_model(num_classes):\n",
        "\n",
        "  class Classifier(tf.keras.Model):\n",
        "    def __init__(self, num_classes):\n",
        "      super(Classifier, self).__init__(name=\"prediction\")\n",
        "      self.encoder = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1', trainable=True)\n",
        "      self.dropout = tf.keras.layers.Dropout(0.1)\n",
        "      self.dense = tf.keras.layers.Dense(num_classes)\n",
        "\n",
        "    def call(self, preprocessed_text):\n",
        "      encoder_outputs = self.encoder(preprocessed_text)\n",
        "      pooled_output = encoder_outputs[\"pooled_output\"]\n",
        "      x = self.dropout(pooled_output)\n",
        "      x = self.dense(x)\n",
        "      return x\n",
        "\n",
        "  model = Classifier(num_classes)\n",
        "  return model"
      ],
      "metadata": {
        "id": "RMGpHhfxwCx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_loader(l, r, length, bert_preprocess_model):\n",
        "  classes = {'fake':0, 'clickbait':1, 'valid':2, 'bias':3}\n",
        "  news={'input_mask':[], 'input_type_ids':[], 'input_word_ids':[]}\n",
        "  labels = []\n",
        "  for i in range(l,r):\n",
        "      print(i)\n",
        "      file = open('./drive/MyDrive/data2/pickle-'+str(i)+'.csv', 'r', encoding='latin-1')\n",
        "      reader = csv.reader(file)\n",
        "      for r in reader:\n",
        "        pre = bert_preprocess_model([np.array([r[0]]), np.array([r[1]])])\n",
        "        news['input_mask'].append(pre['input_mask'])\n",
        "        news['input_type_ids'].append(pre['input_type_ids'])\n",
        "        news['input_word_ids'].append(pre['input_word_ids'])\n",
        "        labels.append(classes[r[2]])\n",
        "      file.close()\n",
        "  news['input_mask'] = tf.reshape(news['input_mask'], (length, 128))\n",
        "  news['input_type_ids'] = tf.reshape(news['input_type_ids'], (length, 128))\n",
        "  news['input_word_ids'] = tf.reshape(news['input_word_ids'], (length, 128))\n",
        "  return (news,labels)"
      ],
      "metadata": {
        "id": "CfUblucUva65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_train = data_loader(0,320,160000,bert_preprocess_model)"
      ],
      "metadata": {
        "id": "x-uEr1XD_EMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_valid = data_loader(320,360,20000,bert_preprocess_model)"
      ],
      "metadata": {
        "id": "UqqvUAK3EbyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_test = data_loader(360,400,20000,bert_preprocess_model)"
      ],
      "metadata": {
        "id": "RA1pzqpbFi7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset_from_tfds(in_memory_ds, train, batch_size, ):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(in_memory_ds)\n",
        "  if train:\n",
        "    dataset = dataset.shuffle(len(dataset))\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  dataset = dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "WxIVUgqnfHEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 3\n",
        "batch_size = 32\n",
        "init_lr = 2e-5\n",
        "\n",
        "train_dataset = load_dataset_from_tfds(news_train, train=True, batch_size=batch_size)\n",
        "steps_per_epoch = 160000 // batch_size\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = num_train_steps // 10\n",
        "\n",
        "validation_dataset = load_dataset_from_tfds(news_test, train=False, batch_size=batch_size, )\n",
        "validation_steps = 20000 // batch_size\n",
        "\n",
        "metrics = tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "classifier_model = build_classifier_model(4)\n",
        "\n",
        "optimizer = optimization.create_optimizer(\n",
        "    init_lr=init_lr,\n",
        "    num_train_steps=num_train_steps,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    optimizer_type='adamw')\n",
        "\n",
        "classifier_model.compile(optimizer=optimizer, loss=loss, metrics=[metrics])"
      ],
      "metadata": {
        "id": "DUsdFgTPygPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"./cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)"
      ],
      "metadata": {
        "id": "udblMzT40rx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_model.fit(\n",
        "    x=train_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=epochs,\n",
        "    validation_steps=validation_steps,\n",
        "    callbacks=[cp_callback])"
      ],
      "metadata": {
        "id": "GM744QTeCu8P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64099195-fa54-4212-d9f3-73f2e441058f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            " 764/5000 [===>..........................] - ETA: 12:01:05 - loss: 1.1402 - accuracy: 0.4929"
          ]
        }
      ]
    }
  ]
}